{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cancer r_isa maladie\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aucune relation trouvée ou relation pas prise en compte par notre systeme !\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aucune relation trouvée ou relation pas prise en compte par notre systeme !\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aucune relation trouvée ou relation pas prise en compte par notre systeme !\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-58a69ba14c9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mrel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mrel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nc1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mncs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0mrel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nc2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mncs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "root_path=\"nltkTools\"\n",
    "pos_tagg = StanfordPOSTagger(root_path + \"/french.tagger\", root_path + \"/stanford-postagger.jar\",encoding='utf8') #instance de la classe StanfordPOSTagger en UTF-8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file = open(\"regles/regle.txt\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "filer_isa = open(\"Nouvelles relations trouvees/r_isa.txt\",\"a\")\n",
    "filer_own = open(\"Nouvelles relations trouvees/r_own.txt\",\"a\")\n",
    "filer_carac = open(\"Nouvelles relations trouvees/r_carac.txt\",\"a\")\n",
    "\n",
    "fileauxavoir = open(\"axiliaires/auxavoir.txt\")\n",
    "auxAvoirlines = fileauxavoir.readlines()\n",
    "fileauxavoir.close()\n",
    "\n",
    "fileauxetre = open(\"axiliaires/auxetre.txt\")\n",
    "auxEtrelines = fileauxetre.readlines()\n",
    "fileauxetre.close()\n",
    "\n",
    "\n",
    "fileText = open(\"texte.txt\")\n",
    "text = fileText.read()\n",
    "fileText.close()\n",
    "\n",
    "phrases = text.split(\".\")\n",
    "\n",
    "\n",
    "for phrase in phrases:\n",
    "    if len(phrase) != 0:\n",
    "        auxavoir = []\n",
    "        auxetre = []\n",
    "\n",
    "        for aux in auxAvoirlines:\n",
    "            aux = aux.strip()\n",
    "            if(len(aux) != 0):\n",
    "                if(len(aux) < 9):\n",
    "                    auxavoir.append(aux)\n",
    "\n",
    "        for aux in auxEtrelines:\n",
    "            aux = aux.strip()\n",
    "            if(len(aux) != 0):\n",
    "                if(len(aux) < 9):\n",
    "                    auxetre.append(aux)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        phrase = phrase.lower()\n",
    "\n",
    "        motokenizze = word_tokenize(phrase)\n",
    "        mottagg = pos_tagg.tag(motokenizze)\n",
    "\n",
    "\n",
    "\n",
    "        #print(mottagg)\n",
    "\n",
    "        tag = \"\"\n",
    "\n",
    "        ncs = \"\"\n",
    "\n",
    "        ncr = \"\"\n",
    "\n",
    "        adj = \"\"\n",
    "\n",
    "        i=1\n",
    "\n",
    "        for va in mottagg:\n",
    "\n",
    "            if(va[1] == 'NC'):\n",
    "\n",
    "                ncs += va[0]+\" \"\n",
    "\n",
    "            if(va[1] == 'ADJ'):\n",
    "                adj += va[0]\n",
    "\n",
    "            if(va[1] == 'V' and va[0] in auxavoir):\n",
    "                tag += \"avoir\"\n",
    "\n",
    "            elif(va[1] == 'V' and va[0] in auxetre):\n",
    "                tag += \"être\"\n",
    "\n",
    "            elif (va[1] == 'NC'):\n",
    "                tag+=va[1]+str(i)+\" \"\n",
    "                i=i+1\n",
    "\n",
    "            else:\n",
    "                tag+=va[1]+\" \"\n",
    "                print(\"\")\n",
    "\n",
    "\n",
    "        tag = tag.lower()\n",
    "        tag = tag.replace(\" \",\"\")\n",
    "\n",
    "        adj = word_tokenize(adj)\n",
    "       \n",
    "        ncs = word_tokenize(ncs)\n",
    "       \n",
    "        regle = \"\"\n",
    "        relation = \"\"\n",
    "        r = \"\"\n",
    "\n",
    "        resultat = \"\"\n",
    "\n",
    "        r_carac = \"r_carac\"\n",
    "        r_isa = \"r_isa\"\n",
    "        r_own = \"r_own\"\n",
    "\n",
    "        rel=\"\"\n",
    "        #print(tag)\n",
    "\n",
    "        #print(ncs)\n",
    "        #print(ncs[0])\n",
    "        #print(adj)\n",
    "\n",
    "        for l in lines: \n",
    "\n",
    "            l = l.strip()\n",
    "            if len(l) > 10:\n",
    "                rl = word_tokenize(l)\n",
    "                rel =rl[-3]+\" \"+rl[-2]+\" \"+rl[-1]\n",
    "                rel =rel.replace(\"nc1\",ncs[0])\n",
    "                rel =rel.replace(\"nc2\",ncs[1])\n",
    "                if len(adj) !=0 :\n",
    "                    rel =rel.replace(\"adj\",adj[0]) \n",
    "                if len(ncs) >2:\n",
    "                    rel =rel.replace(\"nc3\",ncs[2])\n",
    "                relation = rel\n",
    "\n",
    "\n",
    "            l = l.replace(\" \",\"\")\n",
    "            if len(l) != 0:\n",
    "                regle=l\n",
    "                token = word_tokenize(regle)\n",
    "                r = token[0]\n",
    "                r = r.replace(\"=\",\"\")\n",
    "                if(r in tag):\n",
    "                    if(r_carac in relation):\n",
    "                            filer_carac.write(relation+\"\\n\") \n",
    "                            resultat = relation\n",
    "                    if(r_isa in relation):\n",
    "                            filer_isa.write(relation+\"\\n\")\n",
    "                            resultat = relation\n",
    "                    if(r_own in relation):\n",
    "                            filer_own.write(relation+\"\\n\")\n",
    "                            resultat = relation\n",
    "                            \n",
    "\n",
    "        print(resultat)  \n",
    "        if(resultat == \"\"):\n",
    "            print(\"Aucune relation trouvée ou relation pas prise en compte par notre systeme !\")\n",
    "\n",
    "        adj=\"\" \n",
    "        relation=\"\"\n",
    "filer_isa.close()\n",
    "filer_own.close()\n",
    "filer_carac.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
